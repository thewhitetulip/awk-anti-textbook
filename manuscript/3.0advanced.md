# Advanced

## File Manipulation

We want to prepare our sales.csv file based on rsales.csv

    code ⏰  awk -F"," -OFS="," 'NR==1 { $1="month"; $2="sales"; print } NR>1 && NF>1 {print}' rsales.csv > sales.csv
    code ⏰  cat sales.csv
    month,sales
    Jan,100
    Feb,200
    March,100
    April,50
    May,900
    June,1000
    July,10
    August,20
    September,456
    October,134
    November,934
    December,545

What we did in the above code:

1. Updated the first and second fields of the header using the first block. If you just assign variables to $1 and $2, their values are changed.
2. In the scond block, we parse _only_ those the non header lines (NR>1) and the not null lines (NF>1)

But there is an obvious problem, the header is not comma seperated. All we have to do is set the output field seperator, OFS.

    code ⏰  awk -v OFS="," -F"," 'NR==1 { $1="month"; $2="sales"; print } NR>1 && NF>1 {print}' rsales.csv > sales.csv

Let's now create a new file by the name s_sales.csv. 

Modify the sales.csv to contain multiple lines for each month and then change the second column so that it should be unique for every month.

Place the lines randomly and not one after the other. A line containing Jan should be followed by some month other than Jan. This is to show that you don't need a sorted file

    code ⏰ sed '1d' sales.csv | sort  -o s_sales.csv

Note: Removes the header and sorts the file


    code ⏰  awk -F"," '{file= $1 ".tcsv"; printf "%s,", $2 > file}' s_sales.csv
    code ⏰  for file in `ls *.tcsv`; do     
                name=`echo $file | awk -F"." '{print $1}'`;     
                `awk -v OFS="," -F"," -v name="$name" '{output="op.txt"; for(i=1;i<NF;i++){x+= $1} print name,x >>output}' $file`;  
            done && rm *.tcsv

    code ⏰  cat op.txt
    April,40
    August,20
    December,1090
    Feb,200
    Jan,400
    July,40
    June,1000
    March,200
    May,80
    November,934
    October,134
    September,1368

    code ⏰  awk '{file=$1".csv"; print > file}' sales.csv

If we want to push any field into a file then we can use the below syntax. This will push all Jan records in Jan.tsv file and so on. This should create one tsv file for every month.

    code ⏰  awk -F, '{file=$1".tsv"; print > file}' sales.csv
    code ⏰  ls *.tsv
    April.tsv       December.tsv    Jan.tsv         June.tsv        May.tsv         October.tsv     month.tsv
    August.tsv      Feb.tsv         July.tsv        March.tsv       November.tsv    September.tsv   sales.tsv
    code ⏰  head April.tsv 
    April,50
    April,50
    April,50
    April,50

Split a file, even number lines should go to even.txt and odd in odd.txt

    code ⏰  awk 'NR%2==1 {print>"odd.txt"} NR%2==0 {print > "even.txt"}' sales.csv 


## Substitution

Replace Jan with January in our file

    code ⏰  awk 'sub("Jan", "January")' sales.csv
    January,100
    January,100
    January,100
    January,100

Because this is sub(), this matches the first occurance of Jan.

    code ⏰  echo Jan Jan | awk '{sub("Jan", "January")}'

Try this out in your shell prompt.

Replace Jan,Feb, Mar with Q1.

    code ⏰  awk -F, '$1 ~ /Jan|Feb|Mar/ {gsub($1, "Q1"); print}' sales.csv
    Q1,100
    Q1,200
    Q1,100
    Q1,100
    Q1,100
    Q1,100
    Q1,100

Note: gsub will make changes to the file _after_ it was read. This does not change the base file.

If we want to change all the months then we can have multiple if statements.

    code ⏰  awk -F, '{if($1~/Jan|Feb|Mar/){gsub($1,"Q1")};  if($1 ~ /April|May|June/){gsub($1,"Q2")} print}' sales.csv 
    month,sales
    Q1,100
    Q1,200
    Q1,100
    Q1,100
    Q1,100
    Q1,100
    Q1,100
    Q2,900
    Q2,50
    Q2,50
    Q2,50
    Q2,50
    September,456
    Q2,1000
    August,20
    July,10
    December,545
    July,10
    October,134
    July,10
    September,456
    July,10
    September,456
    November,934
    December,545

gsub() also works if you give a regex as the first argument. That'll replace all the strings matching the regex with the target field.

The best example of this is let's say we have to anonymize a bunch of webserver logs. Those logs contain multiple fields in a line IP address, username, link visited.

    code ⏰  awk -F, 'gsub("[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}","NULL") {print}' server.log 
    NULL,therock,/Download,NULL
    NULL,therock,/Download,NULL
    NULL,therock,/Download,NULL
    NULL,therock,/Download,NULL
    NULL,therock,/Download,NULL
    NULL,therock,/Download,NULL
    NULL,therock,/Download,NULL
    NULL,rock,/,NULL

We used gsub(), that's why it substituted the IP address which came twice in one line.

If we had used sub() it'd have replaced the first occurance of an IP address.

## Manipulating the fields themselves

Let's create a new input data file.

Name it marks.csv. It contains the marks in Maths, Science and English for roll numbers 1,2 and 3. The Number field is nothing but the roll number of that student.

    number,maths,science,english
    1,100,100,100
    2,90,85,65
    3,99,88,77

Here, we created a new field, $5 and did a sum of the other numeric fields. You can create as many fields using any maths formula you want like sum, average etc.

    code ⏰  awk -F, 'NR>1{$5=$2+$3+$4; print $1,$5}' marks.csv 
    1 300
    2 240
    3 264

You can also set a particular field to any value which you want to.

Here, awk replaces the second field to 299 after it reads the file i.e. the input file is not updated.

    code ⏰  awk -F, 'NR>1{$2=299; print }' marks.csv 
    1 299 100 100
    2 299 85 65
    3 299 88 77

Print entire field using for 

    code ⏰  awk -F, '{for(i=1;i<=NF;i++){print "Field "i " is " $i}}' marks.csv 
    Field 1 is Number
    Field 2 is Maths
    Field 3 is Science
    Field 4 is English
    Field 1 is 1
    Field 2 is 100
    Field 3 is 100
    Field 4 is 100
    Field 1 is 2
    Field 2 is 90
    Field 3 is 85
    Field 4 is 65
    Field 1 is 3
    Field 2 is 99
    Field 3 is 88
    Field 4 is 77

## Writing to a file from within awk

Moving the sales.csv file into two files. First file contains the first column, second file contains the second column.

    code ⏰  awk -F, '{print $1 > "first.csv"}'  sales.csv

Creates a file called first.csv

    code ⏰  awk -F, '{print $2 > "second.csv"}'  sales.csv

Creates a file called second.csv


The redirect operator works in the same way inside an awk program like it does in the shell.

Please note that the file name should be enclosed in double quotes if it is a string. 

    code ⏰  awk -F, '{print $1 > 1".csv"}'  sales.csv
    awk: syntax error at source line 1
      context is
	    {print $1 > >>>  1".csv" <<<

    awk: illegal statement at source line 1



##### Links

-[Previous section](2.0regex.md)

-[Next section](4.0.md)